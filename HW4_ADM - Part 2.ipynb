{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Find the duplicates!\n",
    "We have a file with 110.000.000 password of 20 characters. We have to define a hash function that associates a value to each string. The goal is to check whether there are some duplicate strings, in 2 cases:\n",
    "\n",
    "- order is not important \"AABA\" = \"AAAB\"\n",
    "- order is important \"AABA\" != \"AAAB\"\n",
    "\n",
    "### Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from bitarray import bitarray\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the duplicates not considering the order of the password's charachters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## LOAD 1 #########\n",
    "# Load all the data \n",
    "with open(\"passwords2.txt\") as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we built the hash function\n",
    "Our basic idea to give a hash code, possibly large and unique, to a password is use the [Fundamental Theorem of Arithmetic](https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic).\n",
    "\n",
    "Shortly it says that each positive integer can be factorized in prime numbers in unique way. This will guarantee us the unicity of hash code for each password, less than the order. The importance of the unicity is explained in the second part.\n",
    "\n",
    "In order to do this we have to associate a prime number to each character so we search all the possible character in the txt file like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty set\n",
    "final_set = set()\n",
    "# iterating over all the passwords...\n",
    "for psw in content:\n",
    "    # remove final \"\\n\"\n",
    "    psw = psw[:-1]\n",
    "    # ... we store all the charachter using the union function in the set library\n",
    "    final_set = final_set.union( set(psw) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 84 different character!\n",
    "\n",
    "After this we built a function that check if a number is prime or not. Because now we need the first 84 prime number to associate them to the 84 characters stored in \"final_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function check if the number in input is prime\n",
    "def is_prime(n):\n",
    "    # if n is equal to 2 is prime\n",
    "    if n == 2:\n",
    "        return True\n",
    "    # we search a divider in all the numbers among 2 and the squared root of the number to check\n",
    "    for i in range(2,int(np.sqrt(n))+1):\n",
    "        # if we find a divider return False because the number isn't prime\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    # at the end if we don't find any divider n is prime so return True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the first part of the next chunk we find the first 84 prime numbers and we store them in \"list_of_prime\".\n",
    "\n",
    "Then we create the association: prime number with character, through the dictionary map_char_prime. It has the following shape:\n",
    "\n",
    "map_char_prime = { \"a\" : 2, \"$\" : 3, \"M\" : 5,  ...  , \"+\" : 433 }\n",
    "\n",
    "Then we save it in order not to repeat these operations each time we run the notebook. We choose json format to store the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_char = len(final_set) # how many prime number we need\n",
    "\n",
    "# find the first \"num_char\" prime number\n",
    "i = 2\n",
    "list_of_prime = []\n",
    "while len(list_of_prime) < num_char:\n",
    "    if is_prime(i):\n",
    "        list_of_prime.append(i)\n",
    "        i += 1\n",
    "        continue\n",
    "    else:\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "# build a map from character to a prime number\n",
    "map_char_prime = defaultdict(int)\n",
    "for i in range(num_char):\n",
    "    map_char_prime[list(final_set)[i]] = list_of_prime[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to associate a huge number to each password. Moreover, for the first task, we want that passwords that has the same character in different order, will have the same hash number. To do this we take advantage of commutativity of the product. \n",
    "\n",
    "Simply the hash code is the product among the prime numbers associated to the character of the password. In this way:\n",
    "- Two equal password, NOT considering the order, have the same hash number\n",
    "- And we can't have 2 different passwords that have the same hash number for the unicity of the factorization in prime numbers\n",
    "\n",
    "The next function take in input the string password and return the hash number described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_password_to_prime( password ):\n",
    "    # create a list with all the character in the string casting it\n",
    "    list_of_char = list(password)\n",
    "    # this number will contain the hash code\n",
    "    final_num = 1\n",
    "    # iterate over the charachters\n",
    "    for character in list_of_char:\n",
    "        # multiplicate the current hash code with the prime number associated to the new charachter\n",
    "        final_num *= map_char_prime[character]\n",
    "    return(final_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to give an hash code to all the passwords and do the hash map. We can't use variable like: list, dataframe or dictionary to store the hash codes, because they will have a size too huge. Then we think that the cheapest choice is to use a binary array (using bitarray library). The map is done making the modulo operation, over the size of this array, of the hash code. In this way we find the position of the binary array. \n",
    "\n",
    "Then if the position is 0 we swap it into 1, but if it's already setted to 1 we have a collision.\n",
    "The collision could be due to a true duplicate or a false positive. However we count all the collision in the variable \"duplicate_count\" and we save the collision position in a set for the next part where we will disinuge between True and False duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a binary array with 10^11 space with all zeros\n",
    "a = bitarray(100000000003)\n",
    "a.setall(0)\n",
    "n_a = len(a)\n",
    "duplicate_pos_set = set()\n",
    "\n",
    "# initialize the counter for the duplicates\n",
    "duplicate_count = 0\n",
    "# search in all the password\n",
    "for psw in content:\n",
    "    # we decide the position in the bit array doing the hash code modulo the length of the bit array\n",
    "    bit_position = map_password_to_prime( psw[:-1] ) % n_a\n",
    "    # if an element is already in that position increase the duplicates counter\n",
    "    if a[bit_position] == True:\n",
    "        duplicate_count += 1\n",
    "        # we store the position with the possible duplicates for the second part of the analysis\n",
    "        duplicate_pos_set.add(bit_position)\n",
    "    # if the element in that position is zero turn it to one ( or True )\n",
    "    else:\n",
    "        a[bit_position] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many possible duplicates we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10050010"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With different attempts we got these results using different number for the modulo:\n",
    "\n",
    "- with 110000017 we have 44318083 possible duplicates\n",
    "- with 500000003 we have 19366303 possible duplicates\n",
    "- with 1000000007 we have 14837676 possible duplicates\n",
    "- with 9000000000 we have 10930362 possible duplicates\n",
    "- with 100000000003 we have 10050010 possible duplicates\n",
    "\n",
    "We know by the given result that there are 50010 false positives. Our hash map is really good it makes a mistake (false duplicates over possible duplicates) of 0.005%. \n",
    "\n",
    "But what about if we don't know the real number of duplicates? How we can understand it? Next we consider this problem. The thing that allows us to do it is that now we have to check only this 10 million of possible duplicates and not all the 110 million!\n",
    "\n",
    "N.B. we use 100000000003 as module because we need a large prime number. According to our [research](https://shanghaiseagull.com/index.php/2017/09/11/why-do-hash-functions-use-prime-numbers/) is better to use a prime number to increase the sparsity of the bitarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in external file the set with all the positions of the possible duplicates in the bitarray. In this way we can avoid repeating the run of all the cells of the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"position_set_1.txt\", \"wb\") as fp:\n",
    "    pickle.dump(duplicate_pos_set, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### LOAD 2 #############\n",
    "with open(\"position_set_1.txt\", \"rb\") as fp:\n",
    "    duplicate_pos_set = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same thing done before, but when we find a bit position that is in the \"duplicate_pos_set\" (all the possible duplicate positions in the bitarray) we save the hash in a set. But if already exists another equal hash code in this set this is a true duplicate then we increase the counter \"real_duplicate\". This thing is possible only because we use a unique hash code for each password unique to less than the order!! Now is explained the choice of the use of the Fundamental Theorem of Arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bitarray(100000000003)\n",
    "a.setall(0)\n",
    "n_a = len(a)\n",
    "\n",
    "# initialize the counter for the real duplicates\n",
    "real_duplicate = 0\n",
    "# and the set will contain all the hash codes \n",
    "duplicate_hash_set = set()\n",
    "\n",
    "# search in all the password\n",
    "for psw in content:\n",
    "    # we decide the position in the bit array doing the hash code modulo the length of the bit array\n",
    "    hash_code = map_password_to_prime( psw[:-1] )\n",
    "    bit_position = hash_code % n_a\n",
    "    # if the bit position is one of those to be checked...\n",
    "    if bit_position in duplicate_pos_set:\n",
    "        # if the hash code is already saved increase the counter of the real duplicates\n",
    "        if hash_code in duplicate_hash_set:\n",
    "            real_duplicate += 1\n",
    "        # otherwise save the hash code in its set\n",
    "        else:\n",
    "            duplicate_hash_set.add(hash_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end this is the real number of duplicate passwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free the cache for the next resarch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(a)\n",
    "del(duplicate_hash_set)\n",
    "del(duplicate_pos_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the duplicates considering the order of the password's charachters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First part\n",
    "The first step is define an hash function that consider the position of the charachter. We know that the possible charachters are in total 84 and we give a number of to digits to each of them. In example:\n",
    "\n",
    "\"a\" -> 10 , \"b\" -> 11, \"c\" -> 12, ... , \"$\" ->94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new dictionray\n",
    "dict_char_position = defaultdict(int)\n",
    "# start from 10 because we want that each charachter has to digits\n",
    "i = 10\n",
    "for charac in list(map_char_prime.keys()):\n",
    "    # create the keys with the character and its values with the next two digits number\n",
    "    dict_char_position[charac] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we give as hash code corresponding to a string, a number that is the substitute of the charachter in invers order of appearing. For instance:\n",
    "\n",
    "\"abc\" -> 121110\n",
    "\n",
    "because \"a\" is 10, \"b\" is 11 and \"c\" is 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hash_position( stringa ):\n",
    "    # this number will contain the final hash code\n",
    "    final_num = 0\n",
    "    # iterate over the charachters in the string and their psition\n",
    "    for pos, el in enumerate(list(stringa)):\n",
    "        # we add to final number the current number of the new charachter\n",
    "        final_num += dict_char_position[el] * ((100)**pos)\n",
    "    return final_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the same thing done before: do the module operation of the hash code with a large number and use the bit array for the hash map. Moreover we save all the position of the possible duplicates in a set for the second part, where we distingue the true and the false duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the bitarray for the hash map with all zeros\n",
    "a_2 = bitarray(100000000003)\n",
    "a_2.setall(0)\n",
    "n_a_2 = len(a_2)\n",
    "# initialize the set will contain the positions of the possible duplicates\n",
    "duplicate_pos_set_2 = set()\n",
    "# initialize a counter for all the possible duplicates when we have a collision\n",
    "duplicate_count_2 = 0\n",
    "for psw in content:\n",
    "    # compute the hash code and find its position in the array\n",
    "    hash_code = map_hash_position( psw[:-1] )\n",
    "    bit_position = hash_code % n_a_2\n",
    "    # if we already have a 1 in this position increase the possible duplicate counter\n",
    "    # and then add this position to the set of the possible duplicates\n",
    "    if a_2[bit_position] == True:\n",
    "        duplicate_count_2 += 1\n",
    "        duplicate_pos_set_2.add(bit_position)\n",
    "    # otherwise swap the 0 into a 1\n",
    "    else:\n",
    "        a_2[bit_position] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many possible duplicates we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5055060"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_count_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the set with all the positions of the possible duplicates in the bitarray. In this way we can avoid repeating the run of all the cells of the first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"position_set_2.txt\", \"wb\") as fp:\n",
    "    pickle.dump(duplicate_pos_set_2, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free the cache for the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(a_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### LOAD 3 #############\n",
    "with open(\"position_set_2.txt\", \"rb\") as fp:\n",
    "    duplicate_pos_set_2 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the number of possible duplicates (true and false positive) is really decrease. We start with 110*10^6 passwords to check and after using the hash map we have only 5055060 elements to verify. We do the same thing done in the previous part using the unicty of the new hash code defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_2 = bitarray(100000000003)\n",
    "a_2.setall(0)\n",
    "n_a_2 = len(a_2)\n",
    "\n",
    "# initialize the counter for the real duplicates\n",
    "real_duplicate_2 = 0\n",
    "# and the set will contain all the hash codes \n",
    "duplicate_hash_set_2 = set()\n",
    "\n",
    "# search in all the password\n",
    "for psw in content:\n",
    "    # we decide the position in the bit array doing the hash code modulo the length of the bit array\n",
    "    hash_code = map_hash_position( psw[:-1] )\n",
    "    bit_position = hash_code % n_a_2\n",
    "    # if the bit position is one of those to be checked...\n",
    "    if bit_position in duplicate_pos_set_2:\n",
    "        # if the hash code is already saved increase the counter of the real duplicates\n",
    "        if hash_code in duplicate_hash_set_2:\n",
    "            real_duplicate_2 += 1\n",
    "        # otherwise save the hash code in its set\n",
    "        else:\n",
    "            duplicate_hash_set_2.add(hash_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_duplicate_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true duplicates number is 5000000 and we have 55060 false positive. With this number we can evaluate the goodness of the hash map that's really good beacuse it makes a mistake (false duplicates over possible duplicates) of 0.01%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(a_2)\n",
    "del(duplicate_hash_set_2)\n",
    "del(duplicate_pos_set_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
